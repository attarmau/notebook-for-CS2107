arr = np.arange(1, 10)
q7_3x3_arr = arr.reshape((3,3))
q7_array = q7_3x3_arr[:, ::-1]
print(q7_array)

arr = np.arange(1, 10)
q7_3x3_arr = arr.reshape((3,3))
q7_array = q7_3x3_arr[::-1]
print(q7_array)

arr = np.arange(1, 10)
q7_3x3_arr = arr.reshape((3,3))
q7_array = q7_3x3_arr[::-1, ::-1]
print(q7_array)

# dynamic_array: 
heros=['spider man','thor','hulk','iron man','captain america']
# 1. Length of the list
print(len(heros))
# 2. Add 'black panther' at the end of this list
heros.append('black panther')
print(heros)
# 3. You realize that you need to add 'black panther' after 'hulk',
# so remove it from the list first and then add it after 'hulk'
heros.remove('black panther')
heros.insert(3,'black panther')
print(heros)

# 4. Now you don't like thor and hulk because they get angry easily :)
#    So you want to remove thor and hulk from list and replace them with doctor strange (because he is cool). Do that with one line of code.
heros[1:3]=['doctor strange']
print(heros)
# The provided code directly replaces the elements at indices 1 and 2 with 'doctor strange' without explicitly deleting the elements first.

# 5. Sort the list in alphabetical order
heros.sort()
print(heroes)


# Download Glove

glove_url = "http://nlp.stanford.edu/data/glove.6B.zip"
destination_file_path = "glove.6B.zip"
urllib.request.urlretrieve(glove_url, destination_file_path)

zip_file_path = "glove.6B.zip"
extracted_dir = "glove_embeddings"

if not os.path.exists(extracted_dir):
    os.makedirs(extracted_dir)

with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extracted_dir)

glove_file_path = os.path.join(extracted_dir, "glove.6B.100d.txt")
if os.path.exists(glove_file_path):
    print("GloVe embeddings file extracted successfully:", glove_file_path)
else:
    print("Error: GloVe embeddings file not found.")

# Use pre-trained GloVe embeddings to do feature engineering 
glove_path = 'glove_embeddings/glove.6B.100d.txt'
glove_embeddings = {}
with open(glove_path, 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vector = np.array(values[1:], dtype='float32')
        glove_embeddings[word] = vector

def normalize_embeddings(embeddings):
    norms = np.linalg.norm(list(embeddings.values()), axis=-1, keepdims=True)
    normalized_embeddings = {word: np.clip(embedding / norms[i], 0, None) for i, (word, embedding) in enumerate(embeddings.items())}
    return normalized_embeddings

def min_max_normalize(embeddings):
    min_val = np.min(embeddings)
    max_val = np.max(embeddings)
    return (embeddings - min_val) / (max_val - min_val)

glove_embeddings_normalized = normalize_embeddings(glove_embeddings)

# Tokenize text in train data using spaCy
nlp = spacy.load('en_core_web_sm')
tokenized_text = [nlp(text) for text in X_train]

# Map tokens to GloVe embeddings
embedded_text = []
for doc in tokenized_text:
    embedded_doc = []
    for token in doc:
        if token.text in glove_embeddings:
            embedded_doc.append(glove_embeddings_normalized[token.text])
        else:
            pass
    embedded_text.append(embedded_doc)

max_seq_length = 100  
X_train_padded = pad_sequences(embedded_text, maxlen=max_seq_length, dtype='float32', padding='post', truncating='post')

X_train_glove = np.array(X_train_padded)

# Tokenize text in test data using spaCy
tokenized_test_text = [nlp(text) for text in X_test]

# Map tokens to GloVe embeddings
embedded_test_text = []
for doc in tokenized_test_text:
    embedded_doc = []
    for token in doc:
        if token.text in glove_embeddings:
            embedded_doc.append(glove_embeddings_normalized[token.text])
        else:
            pass
    embedded_test_text.append(embedded_doc)

X_test_padded = pad_sequences(embedded_test_text, maxlen=max_seq_length, dtype='float32', padding='post', truncating='post')
X_test_glove = np.array(X_test_padded)

# Train the Naive Bayes model with the best hyperparameters using randomized search

X_train_flat = X_train_glove.reshape(X_train_glove.shape[0], -1)
X_test_flattened = X_test_glove.reshape(X_test_glove.shape[0], -1)
naive_bayes_model = MultinomialNB()

param_dist = {'alpha': uniform(0.1, 10)}
random_search = RandomizedSearchCV(naive_bayes_model, param_distributions=param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)
random_search.fit(X_train_flat, y_train)
best_alpha = random_search.best_params_['alpha']
print("Best Alpha of randomized search:", best_alpha)

naive_bayes_model_tuned = MultinomialNB(alpha=best_alpha)

# Use randomized search to find the best hyperparameters for BaggingClassifier
param_grid = {
    'n_estimators': [10, 50, 100],
    'max_samples': [0.5, 0.7, 1.0],
    'max_features': [0.5, 0.7, 1.0],
    'random_state': [42]
}

bagging_classifier = BaggingClassifier(base_estimator=naive_bayes_model_tuned, random_state=42)
grid_search = GridSearchCV(bagging_classifier, param_grid, cv=StratifiedKFold(n_splits=5), scoring='accuracy')
X_train_flat = X_train_glove.reshape(X_train_glove.shape[0], -1)
grid_search.fit(X_train_flat, y_train)
best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best Parameters:", best_params)

# Use tuned BaggingClassifier on my tuned MultinomialNB
best_estimator.fit(X_train_flat, y_train)

# Holdout validation
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_flat, y_train, test_size=0.2, random_state=42)
best_estimator.fit(X_train_split, y_train_split)
val_predictions = best_estimator.predict(X_val_split)
val_accuracy = accuracy_score(y_val_split, val_predictions)
print("Validation Accuracy:", val_accuracy)

# Predict probabilities for the test data
test_predictions_proba = best_estimator.predict_proba(X_test_flattened)
test_predictions_proba_positive = test_predictions_proba[:, 1]
